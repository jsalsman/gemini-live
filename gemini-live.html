<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Gemini Live Voice to Text Realtime Stream</title>
    <!-- By Jim Salsman, April 2025. Released under the free MIT License. -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        body {
            font-family: sans-serif;
            display: flex;
            flex-direction: column;
            height: 100vh;
            margin: 0;
            overflow: hidden; /* Prevent body scroll */
            background-color: #f4f4f4;    
        }
        #controls {
            padding: 15px;
            background-color: #4ed6c934;
            border-bottom: 1px solid #ccc;
            text-align: center;
            position: relative;
        }
        #controls-content {
            display: flex;
            align-items: center;            
            justify-content: flex-end;
            width: 100%;            
            text-align: center;
            gap: 5px; /* Add gap between title and button */
        }
        #toggleStream {
            padding: 10px 20px;
            font-size: 1em;
            cursor: pointer;
            background-color: #4CAF50;
            color: white;
            border: none;
            border-radius: 5px;
        }
        #sendImage {   
            background-color: blue;
            display: none; /* Initially hidden */
            border: none;
            color: white;
            font-size: 1em;
            padding: 10px 20px;
            border-radius: 5px;
        }
        #toggleStream.stop {
            background-color: #f44336;
        }
        #title {
            font-size: 1.2em;
            border-radius: 5px;
            margin-right: auto; /* Push the title to the left */
        }
        #controls-content > label, #controls-content > input {
            margin-left: 5px;
        }
        #debugContainer {
            margin-right: auto; /* Push the container to the left */
            text-align: center; /* Center content within the container */
        }

        #output-container {
            flex-grow: 1; /* Takes remaining height */
            padding: 10px;
            padding-bottom: 30px;
            overflow-y: auto; /* Enables scrolling */
            background-color: #fff;
            border: 1px solid #ddd;
            margin: 8px;
            border-radius: 5px;
        }
        #output {
            white-space: pre-wrap; /* Preserve whitespace and wrap lines */
            word-wrap: break-word; /* Break long words */
            font-family: sans-serif;
            font-size: 0.9em;
        }
        #output p {
            margin-bottom: -0.5em; /* Remove vertical whitespace */
            margin-top: -0.2em;
        }
        #output ul, #output ol, #output li {
            margin-top: -0.5em;
            margin-bottom: -0.5em; /* Remove vertical whitespace */
        }
        #output pre {
            margin: 0; /* Remove vertical whitespace */
            white-space: pre-wrap; /* Preserve whitespace and wrap lines */
        }
        .output-image {
            max-height: 50vh;
        }
        .error {
            color: red;
            font-weight: bold;
        }
        .info {
            color: blue;
            font-style: italic;
        }
    </style>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
</head>
<body>
    <div id="controls">
        <div id="controls-content">
            <h1 id="title">Gemini Live<br>Voice to Text Stream</h1>
            <div id="debugContainer">
                <a href="https://github.com/jsalsman/gemini-live"
                    target="_blank" style="text-decoration: none;">This
                    code is on GitHub.</a><br><br>
                <input type="checkbox" id="debugCheckbox">
                <label for="debugCheckbox"><small>Debug<br>
                    &nbsp;&nbsp;to console</small></label>
            </div>
           <button id="toggleStream">Start<br>
            Listening</button>
           <button id="sendImage">Send<br>
            Image</button>
        </div>
    </div>

    <div id="output-container">
        <pre id="output"></pre>
    </div>

    <script type="module">
        // Import necessary modules
        import { marked } from 'https://esm.sh/marked'; // Markdown
        import markedKatex from 'https://esm.sh/marked-katex-extension'; // LaTeX
        import { GoogleGenAI, Modality } from 'https://esm.run/@google/genai';

        // Docs: https://ai.google.dev/gemini-api/docs/live
        // https://googleapis.github.io/js-genai/main/index.html
        // https://github.com/googleapis/js-genai

        // --- Configuration ---
        const MODEL_NAME = "gemini-2.0-flash-live-001"; // Realtime/Live model
        const TARGET_SAMPLE_RATE = 16000; // Gemini requires 16kHz audio
        const SYSTEM_PROMPT = 'Please be a helpful assistant and kind conversationalist. ' +
            'Respond to the user(s) directly by answering with relevant information ' +
            'in full detail. When you do not have relevant information, use your ' +
            'Google Search tool to search for it. You can use Markdown and LaTeX. ' +
            "Do not merely describe the user(s)' utterances. Do not include timestamps.";

        // --- DOM Elements ---
        const toggleButton = document.getElementById('toggleStream');
        const outputArea = document.getElementById('output');
        const sendImageButton = document.getElementById('sendImage');

        // --- State Variables ---
        let genAI;
        let liveSession = null;
        let mediaStream = null;
        let audioContext = null;
        let audioSource = null;
        let audioWorkletNode = null;
        let isStreaming = false;        
        let currentTurnText = "";
        let totalBytesSent = 0;
        let API_KEY = null;
        
        function getCookie(name) {
            const value = `; ${document.cookie}`;
            const parts = value.split(`; ${name}=`);
            if (parts.length === 2) return parts.pop().split(';').shift();
        }

        function checkApiKey() {
             API_KEY = getCookie('gemini_api_key');
             if (!API_KEY) {
                liveOutput("API Key not set. Please reload this page to enter it.", 'error');
                return false;
            }
            return true;
        }
            
        // --- Debug Checkbox Logic ---
        const debugCheckbox = document.getElementById('debugCheckbox');
        let debugMode = false; // Flag to control debug logging
        // Add an event listener to the checkbox to toggle debug mode
        debugCheckbox.addEventListener('change', () => {
            debugMode = debugCheckbox.checked;
        });

        marked.use(markedKatex({ throwOnError: false }));

        function liveOutput(text, type = 'text') {
            const line = document.createElement('div');
            line.innerHTML = marked.parse(text).replace(
                '<a href=', '<a TARGET="_blank" href='); // all links open in new tab
            line.classList.add('output');
            if (type === 'error') {
                line.classList.add('error');
                console.error(text);
            } else if (type === 'info') {
                line.classList.add('info');
                console.log(text);
            } else {
                 line.classList.add('text');
            }
            if (type !== 'error' && type !== 'info') { // Only log "Gemini:" if not error or info
                console.log("Gemini:", text.replace('\n&nbsp;', ''));
            }
            outputArea.appendChild(line);
            outputArea.parentNode.scrollTop = outputArea.parentNode.scrollHeight;
        }

        function arrayBufferToBase64(buffer) {
            let binary = "";
            const bytes = new Uint8Array(buffer);
            const len = bytes.byteLength;
            for (let i = 0; i < len; i++) {
                binary += String.fromCharCode(bytes[i]);
            }
            return window.btoa(binary);
        }

        // --- Audio Worklet ---
        const AudioRecordingWorklet = `
            class AudioProcessingWorklet extends AudioWorkletProcessor {
                buffer = new Int16Array(2048);
                bufferWriteIndex = 0;

                constructor() {
                  super();
                    this.port.onmessage = (event) => {
                        // console.log("Worklet received message:", event.data);
                    };
                }

                process(inputs, outputs, parameters) {
                    if (inputs.length > 0 && inputs[0].length > 0) {
                        const channelData = inputs[0][0];
                        this.processChunk(channelData);
                    }
                    return true; // Keep processor alive
                }

                sendAndClearBuffer() {
                    if (this.bufferWriteIndex > 0) {
                        const dataToSend = this.buffer.slice(0, this.bufferWriteIndex);
                        this.port.postMessage({
                            eventType: "audioData",
                            audioData: dataToSend.buffer // Send ArrayBuffer
                        }, [dataToSend.buffer]); // Transfer buffer ownership for efficiency
                        this.bufferWriteIndex = 0;
                    }
                }

                processChunk(float32Array) {
                    for (let i = 0; i < float32Array.length; i++) {
                        const clampedValue = Math.max(-1.0, Math.min(1.0, float32Array[i]));
                        const int16Value = Math.floor(clampedValue * 32767);
                        this.buffer[this.bufferWriteIndex++] = int16Value;
                        if (this.bufferWriteIndex >= this.buffer.length) {
                            this.sendAndClearBuffer();
                        }
                    }
                }
            }
            registerProcessor('audio-processing-worklet', AudioProcessingWorklet);
        `;

        // --- Core Streaming Logic ---

        async function startStreaming() {
            if (isStreaming) return;
            if (!checkApiKey()) {
                return;
                
            }


            isStreaming = true; // Set streaming flag early
            toggleButton.innerHTML = "Stop<br>Listening";
            toggleButton.classList.add('stop');
         
            try {
                // Step 1: Initialize GoogleGenAI Client
                genAI = new GoogleGenAI({ apiKey: API_KEY });
                try {
                    const modelInfo = await genAI.models.get({ model: MODEL_NAME });
                    console.log("Gemini model info:", modelInfo);                
                } catch (testError) {
                    // Clear the cookie
                    document.cookie = 'gemini_api_key=; expires=Thu, 01 Jan 1970 00:00:00 UTC; path=/;';
                    // Redirect to root
                    window.location.href = '/';
                    return;
                }

                // Step 2: Get Microphone Access
                mediaStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: TARGET_SAMPLE_RATE,
                        echoCancellation: false,
                        noiseSuppression: true
                    }
                });
                
                // Step 3: Create Audio Context and Source
                audioContext = new AudioContext({ sampleRate: TARGET_SAMPLE_RATE });
                // Resume context if suspended (often needed after page load)
                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                }
                audioSource = audioContext.createMediaStreamSource(mediaStream);

                // 4. Set up Audio Worklet
                const workletBlob = new Blob([AudioRecordingWorklet], { type: 'application/javascript' });
                const workletURL = URL.createObjectURL(workletBlob);
                try {
                    await audioContext.audioWorklet.addModule(workletURL);
                } catch (e) {
                    liveOutput(`Error adding AudioWorklet module: ${e.message}. Make sure you are serving this page over HTTPS or localhost.`, 'error');
                    throw e; // Re-throw to be caught by outer catch
                }
                audioWorkletNode = new AudioWorkletNode(audioContext, 'audio-processing-worklet');
  
                // Step 4: Connect Audio Nodes: Mic Source -> Worklet
                audioSource.connect(audioWorkletNode);

                // Step 5: Connect to Gemini Live API
                // Assign to liveSession *after* connection is successful
                liveSession = await genAI.live.connect({
                    model: MODEL_NAME,
                    audioConfig: { targetSampleRate: TARGET_SAMPLE_RATE},
                    systemInstruction: SYSTEM_PROMPT,
                    config: {
                        temperature: 0,
                        tools: [
                            {codeExecution: {}},
                            {googleSearch: {}},
                            ],
                        responseModalities: [Modality.TEXT]
                    },
                    callbacks: {
                        onopen: () => {
                            // The connection to Gemini is open; audio processing and sending can begin
                            sendImageButton.style.display = 'inline-block'; // Show the button
                            liveOutput("Connected to Gemini. Listening...", 'info');
                        },
                        onmessage: (message) => {
                            if (debugMode) {
                                console.log("Debug message:", message);
                            }

                            if (message.serverContent?.modelTurn?.parts?.length > 0) {  
                                const part = message.serverContent.modelTurn.parts[0];

                                if (part.inlineData && part.inlineData.mimeType.startsWith('image/')) {
                                    try {
                                        // Handle image data
                                        const img = document.createElement('img');       
                                        img.classList.add('output-image'); // Add CSS class for styling
                                        img.src = `data:${part.inlineData.mimeType};base64,${part.inlineData.data}`;
                                        liveOutput("Image from Gemini:", "info")
                                        outputArea.appendChild(img);
                                        outputArea.parentNode.scrollTop = outputArea.parentNode.scrollHeight;
                                    } catch (renderError) {
                                        liveOutput(`Error rendering image: ${renderError.message}`, 'error');
                                    }
                                } else if (part.executableCode) {
                                    liveOutput("**Python code:**\n```\n" + part.executableCode.code + " ```");
                                } else if (part.codeExecutionResult) {
                                    const result = part.codeExecutionResult;
                                    liveOutput("**Output:** (" + result.outcome + ")\n``` \n" + result.output + " ```");
                                } else {
                                    const textPart = message.serverContent.modelTurn.parts.find(part => part.text);
                                    if (textPart && textPart.text.trim()) {
                                        currentTurnText += textPart.text;
                                    }
                                }

                            }
                            if (message.serverContent?.turnComplete || message.generationComplete) {
                                // Finalize text when a turn was completed
                                if (currentTurnText.trim().length > 0) {
                                    liveOutput(currentTurnText + "\n&nbsp;");
                                }
                                currentTurnText = ""; // Reset for next turn
                            }
                            if (message.error) {
                                liveOutput(`Gemini Error: ${message.error.message || JSON.stringify(message.error)}`, 'error');
                            }
                        },
                        onerror: (errorEvent) => {
                            // Use errorEvent directly if it's an Error object, otherwise stringify
                            const errorMessage = errorEvent instanceof Error ? errorEvent.message : JSON.stringify(errorEvent);
                            liveOutput(`Gemini connection error: ${errorMessage}`, 'error');
                            stopStreaming(); // Stop on connection error
                        },
                        onclose: (closeEvent) => {
                            // Only call stopStreaming if the closure was unexpected while we were actively streaming
                            if (isStreaming) {
                                liveOutput("Connection closed unexpectedly.", 'error');
                                stopStreaming();
                            }
                         },
                    },
                });

                // Step 6: Handle Audio Data from Worklet
                audioWorkletNode.port.onmessage = (event) => {
                    // Check if it's audio data, if the session exists, and if we are still streaming
                    if (event.data.eventType === 'audioData' && liveSession && isStreaming) {
                        const audioDataBuffer = event.data.audioData;
                        const base64AudioData = arrayBufferToBase64(audioDataBuffer);
                        
                        try {
                            // Send audio chunk to Gemini
                            liveSession.sendRealtimeInput({
                                media: {
                                    data: base64AudioData,
                                    mimeType: `audio/pcm;rate=${TARGET_SAMPLE_RATE}`
                                }
                            });
                            totalBytesSent += audioDataBuffer.byteLength;
                            if (debugMode && (totalBytesSent - 4096) % 163840 === 0) { // every ~5 seconds
                                console.log("Audio sent", totalBytesSent, "bytes,",
                                    Math.round(totalBytesSent / 32000), "seconds");
                            }
                        } catch (sendError) {
                            liveOutput(`Error sending audio data: ${sendError.message}`, 'error');
                            // Optionally stop streaming if sending fails repeatedly
                            // stopStreaming();
                        }
                    }
                };

            } catch (error) {
                liveOutput(`Error starting stream: ${error.message || error}`, 'error');
                await stopStreaming(); // Ensure cleanup happens even on startup error
            }
        }

        async function stopStreaming() {
            // Prevent multiple stop calls overlapping
            if (!isStreaming && !liveSession && !mediaStream && !audioContext) {
                console.log("Stop called but already stopped/cleaned up.");
                return;
            } 
            const wasStreaming = isStreaming; // Keep track if we were actively streaming
            isStreaming = false; // Set flag immediately to stop sending data

            // Step 7: Close Gemini Session
            if (liveSession) {
                try {
                    liveSession.close();
                } catch (e) {
                    liveOutput(`Error closing Gemini session: ${e.message}`, 'error');
                } finally {
                    liveSession = null;
                }
            }

            // Step 8: Stop Audio Processing
            if (audioWorkletNode) {
                audioWorkletNode.disconnect();
                audioWorkletNode = null;
            }
            if (audioSource) {
                audioSource.disconnect();
                audioSource = null;
            }            
            // Step 9: Stop Media Stream Tracks
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            // Step 10: Close Audio Context
            if (audioContext && audioContext.state !== 'closed') {
                try {
                    await audioContext.close();
                } catch (e) {
                    liveOutput(`Error closing AudioContext: ${e.message}`, 'error');
                } finally { audioContext = null;
                }
            }

            // Step 11: Reset UI
            toggleButton.innerHTML = "Start<br>Listening";
            toggleButton.classList.remove('stop');
            sendImageButton.style.display = 'none'; // Hide the button
            genAI = null; // Clear the client instance
            if (wasStreaming) {
                liveOutput("Stopped listening.\n", 'info');
            } 
           
        }

        // --- Event Listener ---
        toggleButton.addEventListener('click', () => {
            if (isStreaming) {
                stopStreaming();
            } else {
                startStreaming();
            }
        });

        // Enhanced "Send Image" button event listener
        document.getElementById('sendImage').addEventListener('click', () => {
            // Create a file input element
            const input = document.createElement('input');
            input.type = 'file';
            input.accept = 'image/*'; // Accept only image files
            
            // Add event listener for file selection
            input.addEventListener('change', (event) => {
                const file = event.target.files[0]; 
                if (file) {                    
                    liveOutput(`Processing image: ${file.name}`, 'info');
                    const reader = new FileReader();
                    reader.onload = async () => {
                        try {
                            let base64Data = reader.result.split(',')[1];
                            let mimeType = file.type;

                            // Downscale if necessary
                            if (base64Data.length > 250 * 1024 * 0.75) { // Approximate base64 size
                                const downscaled = await downscaleImage(base64Data, mimeType, 250 * 1024);
                                base64Data = downscaled.base64;
                                mimeType = downscaled.mimeType;
                                liveOutput(`Image downscaled to ${(base64Data.length * 0.75 / 1024).toFixed(2)}KB`, 'info');
                            }

                            // Display image in output area
                            const img = document.createElement('img');
                            img.classList.add('output-image');
                            img.src = `data:${mimeType};base64,${base64Data}`;
                            outputArea.appendChild(img);
                            outputArea.parentNode.scrollTop = outputArea.parentNode.scrollHeight;

                            // Send image to stream
                            sendImageToStream(base64Data, mimeType);

                        } catch (error) {
                            liveOutput(`Error processing image: ${error.message}`, 'error');
                        }
                    };
                    reader.onerror = (error) => { liveOutput(`Failed to read file: ${error}`, 'error'); };
                    reader.readAsDataURL(file);
                }
            });
            
            // Trigger the file input dialog
            input.click();
        });

        // Helper function for downscaling
        function downscaleImage(base64, mimeType, maxSize) {
            return new Promise((resolve, reject) => {
                const img = new Image();
                img.onload = () => {
                    let width = img.width;
                    let height = img.height;
                    let newWidth = width;
                    let newHeight = height;

                    // Calculate scaling factor to fit within maxSize
                    const canvas = document.createElement('canvas');
                    const ctx = canvas.getContext('2d');

                    // Binary search for the right quality
                    let low = 0.1;
                    let high = 1.0;
                    let quality = 0.8;  // Initial guess
                    let result = null;

                    for (let i = 0; i < 10; i++) {  // 10 iterations of binary search
                        canvas.width = newWidth;
                        canvas.height = newHeight;
                        ctx.drawImage(img, 0, 0, newWidth, newHeight);
                        const newBase64 = canvas.toDataURL(mimeType, quality).split(',')[1];
                        const newSize = newBase64.length * 0.75;

                        if (newSize <= maxSize) {
                            result = { base64: newBase64, mimeType: mimeType };
                            low = quality;  // Try for better quality
                        } else {
                            high = quality;
                            // Scale down image dimensions to reduce size.
                            const scaleFactor = Math.sqrt(maxSize / newSize);
                            newWidth = Math.floor(newWidth * scaleFactor);
                            newHeight = Math.floor(newHeight * scaleFactor);
                        }
                        quality = (low + high) / 2;
                    }

                    if (result) {
                        resolve(result);
                    } else {
                        // Should not happen after scaling image dimensions
                        reject(new Error("Could not downscale image to the required size."));
                    }
                };
                img.onerror = () => reject(new Error("Failed to load image for downscaling."));
                img.src = `data:${mimeType};base64,${base64}`;
            });
        }

        // Function to send the image to the stream
        async function sendImageToStream(base64Image, mimeType = 'image/jpeg') {
            if (!liveSession) {
                liveOutput('Live session not available. Please start listening first.', 'error');
                return;
            }
            if (!isStreaming) {
                liveOutput('Streaming is not active. Please start listening first.', 'error');
                return;
            }
            
            try {
                // Debug information
                if (debugMode) {
                    console.log(`Sending image (${(base64Image.length * 0.75 / 1024).toFixed(2)}KB) with MIME type: ${mimeType}`);
                }
                
                // Send the image as a completed turn
                await liveSession.sendClientContent({
                    turns: [{
                        role: 'user',
                        parts: [{
                            inlineData: {
                                mimeType: mimeType,
                                data: base64Image
                            }
                        }]
                    }],                    
                    turnComplete: true
                });
                
                liveOutput('Image sent successfully! Waiting for Gemini to process...', 'info');
            } catch (error) {
                // Handle the error appropriately
                liveOutput(`Error sending image: ${error.message || JSON.stringify(error)}`, 'error');                
            }
        }

        // Add safety net for page unload
        if (checkApiKey()) {
            liveOutput("Click 'Start Listening' to begin.", 'info');
        }
        window.addEventListener('beforeunload', () => {
            if (isStreaming) {
                stopStreaming(); // Attempt cleanup if user navigates away
            }
        });
    </script>
</body>
</html>
